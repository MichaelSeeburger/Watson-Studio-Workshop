{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"border: none\" align=\"left\">\n",
    "   <tr style=\"border: none\">\n",
    "      <th style=\"border: none\"><font face=\"verdana\" size=\"5\" color=\"black\"><b>Part 1: Use Spark and Python to Predict Equipment Purchase: Train & Deploy Model</b></th>\n",
    "      <th style=\"border: none\"><img src=\"https://github.com/pmservice/customer-satisfaction-prediction/blob/master/app/static/images/ml_icon_gray.png?raw=true\" alt=\"Watson Machine Learning icon\" height=\"40\" width=\"40\"></th>\n",
    "   </tr>\n",
    "   <tr style=\"border: none\">\n",
    "       <th style=\"border: none\"><img src=\"https://github.com/pmservice/wml-sample-models/blob/master/spark/product-line-prediction/images/products_graphics.png?raw=true\" alt=\"Icon\"> </th>\n",
    "   </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to perform data analysis on classification problem using <a href=\"http://spark.apache.org/docs/2.3.0/api/python/pyspark.ml.html\" target=\"_blank\" rel=\"noopener no referrer\">PySpark ML package</a>.\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook is compatible with Python 3.6 and Apache® Spark 2.x.\n",
    "\n",
    "You will use a publicly available data set, **GoSales Transactions for Naive Bayes Model**, which details anonymous outdoor equipment purchases. This data set will be used to predict clients' interests in terms of product line, such as golf accessories, camping equipment, and so forth.\n",
    "\n",
    "**Note**: In this notebook, we use the GoSales data available to the <a  href=\"https://apsportal.ibm.com/exchange-api/v1/entries/8044492073eb964f46597b4be06ff5ea/data?accessKey=9561295fa407698694b1e254d0099600\" target=\"_blank\" rel=\"noopener no referrer\">Watson Studio Community</a>.\n",
    "\n",
    "We should have already refined the data in part 1 of the task so that it is clean and usable, explored the data to look at the attributes and their relations.\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "-  Load a CSV file into an Apache® Spark DataFrame.\n",
    "-  Prepare data for training and evaluation.\n",
    "-  Create an Apache® Spark machine learning pipeline.\n",
    "-  Train and evaluate a model.\n",
    "-  Store a pipeline and model in the Watson Machine Learning (WML) repository.\n",
    "-  Deploy a model for online scoring via the Watson Machine Learning (WML) API.\n",
    "-  Score the model using sample data via the Watson Machine Learning (WML) API.\n",
    "-  Explore the prediction results.\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "1.\t[Set up the environment](#setup)\n",
    "2.  [Load the data](#load)\n",
    "3.  [Explore the data](#explore)\n",
    "4.\t[Build an Apache® Spark machine learning model](#model)\n",
    "5.\t[Store the model in the WML repository](#persistence)\n",
    "6.\t[Deploy and score in a Cloud](#scoring)\n",
    "7.\t[Summary and next steps](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 1. Set up the environment\n",
    "\n",
    "Before you use the sample code in this notebook, you must make sure that you are using `Python 3.6 with Spark` kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "## 2. Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will load the data as an Apache® Spark DataFrame and explore the data.\n",
    "\n",
    "1. Go to the 'Find and add data' panel on the right hand side panel\n",
    "2. Find the shaped data asset 'Outdoor_Equipment_Sales.csv_shaped.csv'\n",
    "3. Click on the 'Insert to code' dropdown\n",
    "4. Select 'Insert SparkSession DataFrame'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part we will copy the data in another dataframe `df` for ease of use in upcoming code cells and have a look at the data types and values. Please ensure to replace the name `df_data_1` with the dataframe name you received from the `Insert to Code` action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_data_1.union(df_data_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"explore\"></a>\n",
    "## 3. Explore the data\n",
    "In this section, you will learn how to:\n",
    "\n",
    "- [3.1 Change data type](#dtype)\n",
    "- [3.2 Visualize data](#vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Change data type<a id=\"dtype\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us have a look at the data types of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the previous steps, the data contains five attributes. PRODUCT_LINE is the one you would like to predict (label).\n",
    "And the other four attributes `GENDER`, `AGE`, `MARITAL_STATUS`, `PROFESSION` are features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute `AGE` is in string type, but this is a numerical data, so we need to convert it to a numerical type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "df = df.withColumn(\"AGE\", df[\"AGE\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "tableView"
     }
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualize data<a id=\"vis\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required library `seaborn` and convert the dataset to Pandas to explore visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df_pd = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see what is the gender distribution in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender = df_pd.GENDER.value_counts()\n",
    "x = df_gender.index.values\n",
    "y = df_gender.values\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=x, y=y, data=df_pd, palette=\"Blues_d\")\n",
    "ax.set_xlabel(\"Gender\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now create a visualization to see how the Marital status distribution is in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Marital Status distribution visualization here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see what is the mutual relation between age, profession and marital status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x=\"AGE\", y=\"PROFESSION\", hue=\"MARITAL_STATUS\", data=df_pd, palette=\"muted\", height=6, aspect=2.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly you can play around visualizing different kinds of charts to understand the dataset better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "## 4. Build an Apache® Spark machine learning model\n",
    "\n",
    "In this section, you will learn how to:\n",
    "\n",
    "- [4.1 Split data](#prep)\n",
    "- [4.2 Build an Apache® Spark machine learning pipeline](#pipe)\n",
    "- [4.3 Train a model](#train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Split data<a id=\"prep\"></a>\n",
    "\n",
    "In this subsection, you will split your data into: \n",
    "- Train data set (80%)\n",
    "- Test data set (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = df.randomSplit([0.8, 0.2], 24)\n",
    "train_data = split_data[0]\n",
    "test_data = split_data[1]\n",
    "\n",
    "print('Number of training records: ' + str(train_data.count()))\n",
    "print('Number of testing records : ' + str(test_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can also try different ratio of split and see how that affects the accuracy of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a different split of dataset here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, your data has been successfully split into two data sets: \n",
    "\n",
    "-  The train data set which is the largest group is used for training.\n",
    "-  The test data set will be used for model evaluation and is used to test the assumptions of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create the pipeline<a id=\"pipe\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection, you will create an Apache® Spark machine learning pipeline and train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step, you need to import the Apache® Spark machine learning modules that will be needed in the subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step, use the `StringIndexer` transformer to convert all string fields into numerical type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexer_label = StringIndexer(inputCol='PRODUCT_LINE', outputCol='label').fit(df)\n",
    "stringIndexer_prof = StringIndexer(inputCol='PROFESSION', outputCol='PROFESSION_IX')\n",
    "stringIndexer_gend = StringIndexer(inputCol='GENDER', outputCol='GENDER_IX')\n",
    "stringIndexer_mar = StringIndexer(inputCol='MARITAL_STATUS', outputCol='MARITAL_STATUS_IX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step, create a feature vector to combine all features (predictors) together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler_features = VectorAssembler(inputCols=['GENDER_IX', 'AGE', 'MARITAL_STATUS_IX', 'PROFESSION_IX'], outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, select the estimator you want to use for classification. `Random Forest` is used in this example.\n",
    "\n",
    "`Random Forest Classifier` can be used with a wide range of hyperparameters. You can tune them to boost your accuracy.\n",
    "More details about the available hyperparameters can be found <a  href=\"http://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.classification.RandomForestClassifier\" target=\"_blank\" rel=\"noopener no referrer\">here</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For starters, try giving different values for `numTrees` and `maxDepth`\n",
    "numTrees = Param(parent='undefined', name='numTrees', doc='Number of trees to train (>= 1).'): Reasonable value between 10 to 50.\n",
    "\n",
    "maxDepth = Param(parent='undefined', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'). Reasonable value between 5 to 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But be careful, high number of trees and high maximum depth means more training time and computing resources will be required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol='label', featuresCol='features', numTrees=20, maxDepth=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, convert the indexed labels back to original labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelConverter = IndexToString(inputCol='prediction', outputCol='predictedLabel', labels=stringIndexer_label.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build the pipeline. A pipeline consists of transformers and an estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rf = Pipeline(stages=[stringIndexer_label, stringIndexer_prof, stringIndexer_gend, stringIndexer_mar, vectorAssembler_features, rf, labelConverter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Train a model<a id=\"train\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can train your Random Forest model by using the previously defined **pipeline** and **train data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the `Random Forest` model, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = pipeline_rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us look at which feature has more importance on deciding the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf.stages[5].featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your **model accuracy** now. Use **test data** to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_rf.transform(test_data)\n",
    "evaluatorRF = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = evaluatorRF.evaluate(predictions)\n",
    "\n",
    "print('Accuracy = {:.2f}%'.format(accuracy*100))\n",
    "print('Test Error = {:.2f}%'.format((1.0 - accuracy)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can tune your model to achieve better accuracy. For simplicity, the tuning step is omitted in this example. Refer to the this <a  href=\"https://github.com/sumit-goyal/Watson-Studio-Workshop/blob/master/notebooks/Part%202%20_%20Use%20Spark%20and%20Python%20to%20Predict.ipynb\" target=\"_blank\" rel=\"noopener no referrer\">notebook</a> for additional details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"persistence\"></a>\n",
    "## 5. Store the model in the WML repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will learn how to use `watson-machine-learning-client` package to store your pipeline and model in the WML repository.\n",
    "\n",
    "Before you use the code in the following section, you must create a <a href=\"https://cloud.ibm.com/catalog/services/machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a lite plan is offered and information about how to create the instance can be found <a href=\"https://dataplatform.ibm.com/docs/content/analyze-data/wml-setup.html\" target=\"_blank\" rel=\"noopener no referrer\">here</a>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Import required package<a id=\"lib\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First, you must import modules of the `watson-machine-learning-client` package.\n",
    "\n",
    "**Note**: Python 3.6 and Apache® Spark version >= 2.1 are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate the Watson Machine Learning service on the IBM Cloud.\n",
    "\n",
    "**Tip**: Authentication information (your credentials) can be found in the <a href=\"https://cloud.ibm.com/docs/resources?topic=resources-service_credentials#viewing-credentials\" target=\"_blank\" rel=\"noopener no referrer\">Service credentials</a> tab of the service instance that you created on the IBM Cloud. \n",
    "\n",
    "If you cannot find the **instance_id** field in **Service Credentials**, click **New credential (+)**  and select premission level `writer` to generate new authentication information. \n",
    "\n",
    "**Action**: Enter your Watson Machine Learning service instance credentials here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_credentials={\n",
    "    'url': '',\n",
    "    'username': '',\n",
    "    'password': '',\n",
    "    'instance_id': ''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = WatsonMachineLearningAPIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Save the pipeline and model<a id=\"save\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection, you will learn how to save pipeline and model artifacts to your Watson Machine Learning instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "published_model_details = client.repository.store_model(model=model_rf, meta_props={'name':'Product line model with Spark'}, training_data=train_data, pipeline=pipeline_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uid = client.repository.get_model_uid(published_model_details)\n",
    "print(model_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get saved model metadata from Watson Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: Use `client.repository.ModelMetaNames.show()` to get the list of available props."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.repository.ModelMetaNames.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"scoring\"></a>\n",
    "## 6. Deploy and score in the WML repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will learn how to create an online deployment, create an online scoring endpoint, and score a new data record using the `watson-machine-learning-client` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You can also use the REST API to deploy and score.\n",
    "For more information about REST APIs, see the [Swagger Documentation](http://watson-ml-api.mybluemix.net/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an online deployment for the published model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_details = client.deployments.create(model_uid, name='Product line model deployment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an online scoring endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_url = client.deployments.get_scoring_url(deployment_details)\n",
    "print(scoring_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can send new scoring records (new data) for which you would like to get predictions. To do that, run the following sample code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_scoring = {'fields': ['GENDER','AGE','MARITAL_STATUS','PROFESSION'],'values': [['M',27,'Single','Student'],['F',68,'Married','Retired']]}\n",
    "\n",
    "client.deployments.score(scoring_url, payload_scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, a 27 year old male student is predicted to be interested in camping equipments (predictedLabel: Camping Equipment). You can also see that a married 68 year old woman is predicted to be interested in Personal Accessories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 7. Summary and next steps     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You successfully completed this notebook! \n",
    " \n",
    "You learned how to use Apache® Spark Machine Learning as well as Watson Machine Learning (WML) API client for model creation and deployment. \n",
    " \n",
    "Check out our [Online Documentation](https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/welcome-main.html?audience=wdp) for more samples, tutorials, documentation, how-tos, and blog posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors\n",
    "\n",
    "**Lukasz Cmielowski**, Ph.D., is an Automation Architect and Data Scientist at IBM with a track record of developing enterprise-level applications that substantially increases clients' ability to turn data into actionable knowledge.  \n",
    "**Jihyoung Kim**, Ph.D., is a Data Scientist at IBM who strives to make data science easy for everyone through Watson Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2017-2019 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 with Spark",
   "language": "python3",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
